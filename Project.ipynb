{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import col, to_date, datediff, when\n",
        "from pyspark.sql.functions import sum as _sum, round, countDistinct\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ETL\").enableHiveSupport().getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Load data and assign column names\n",
        "\n",
        "logistics_df = spark.read.csv(\"/staging_zone/logistics/logistics_data\", header=False, inferSchema=True) \\\n",
        "    .toDF(\"logistics_id\", \"order_id\", \"estimated_delivery_date\", \"actual_delivery_date\", \"shipping_cost\", \"warehouse_id\")\n",
        "\n",
        "order_lines = spark.read.csv(\"/staging_zone/staging_orderlines/staging_order_lines_data\", header=False, inferSchema=True) \\\n",
        "    .toDF(\"order_id\", \"product_id\", \"quantity\", \"price\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "# Inspect schema and sample data for logistics\n",
        "\n",
        "logistics_df.printSchema()\n",
        "logistics_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "# Inspect schema and sample data for order lines\n",
        "\n",
        "order_lines.printSchema()\n",
        "order_lines.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Clean and transform logistics dates: \n",
        "\n",
        "logistics_df = logistics_df.filter(col(\"estimated_delivery_date\").isNotNull() & col(\"actual_delivery_date\").isNotNull()) \\\n",
        "    .withColumn(\"estimated_delivery_date\", to_date(col(\"estimated_delivery_date\"), \"yyyy-MM-dd\")) \\\n",
        "    .withColumn(\"actual_delivery_date\", to_date(col(\"actual_delivery_date\"), \"yyyy-MM-dd\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Create logistics fact table:\n",
        "\n",
        "logistics_fact = logistics_df \\\n",
        "    .withColumn(\"delivery_time_delta_days\", datediff(col(\"actual_delivery_date\") ,col(\"estimated_delivery_date\"))) \\\n",
        "    .withColumn(\"is_late_delivery\", when(col(\"delivery_time_delta_days\") > 0, 1).otherwise(0)) \\\n",
        "    .select(\"order_id\", \"delivery_time_delta_days\", \"is_late_delivery\", \"shipping_cost\", \"warehouse_id\")\n",
        "    \n",
        "logistics_fact.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "# Join logistics fact with order lines to get revenue\n",
        "\n",
        "logistics_fact_joined = logistics_fact.join(order_lines, on=\"order_id\", how=\"inner\") \\\n",
        "                        .withColumn(\"total_revenue\",round(col(\"quantity\") * col(\"price\"), 2))\n",
        "                        \n",
        "logistics_fact_joined.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "# Check if an order is served by multiple warehouses\n",
        "\n",
        "order_warehouse_check = logistics_fact.groupBy(\"order_id\") \\\n",
        "                        .agg(countDistinct(\"warehouse_id\").alias(\"warehouse_count\")) \\\n",
        "                        .withColumn(\"is_multi_warehouse\", when(col(\"warehouse_count\") > 1, 1).otherwise(0)) \\\n",
        "                        .filter(col(\"warehouse_count\") > 1) \n",
        "                        \n",
        "order_warehouse_check.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Aggregate order-level summary:\n",
        "\n",
        "order_summary = logistics_fact_joined.groupBy(\"order_id\", \"warehouse_id\", \"is_late_delivery\", \"shipping_cost\") \\\n",
        "    .agg(round(sum(\"total_revenue\"), 2).alias(\"total_revenue_per_order\")) \\\n",
        "    .orderBy(\"order_id\")\n",
        "\n",
        "order_summary.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Aggregate warehouse-level summary:\n",
        "\n",
        "warehouse_summary = order_summary.groupBy(\"warehouse_id\") \\\n",
        "    .agg(\n",
        "        round(_sum(\"total_revenue_per_order\"), 2).alias(\"total_revenue\"),\n",
        "        round(_sum(\"shipping_cost\"), 2).alias(\"total_shipping_cost\"),\n",
        "        sum(\"is_late_delivery\").alias(\"late_deliveries\"),\n",
        "        countDistinct(\"order_id\").alias(\"total_orders\")) \\\n",
        "    .withColumn(\"late_delivery_rate\", round((col(\"late_deliveries\") / col(\"total_orders\")) * 100, 2)) \\\n",
        "    .withColumn(\"avg_shipping_cost_per_order\", round(col(\"total_shipping_cost\") / col(\"total_orders\"), 2)) \\\n",
        "    .orderBy(\"warehouse_id\")\n",
        "\n",
        "warehouse_summary.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Warehouse efficiency analysis: calculate ratio of shipping cost to total revenue\n",
        "\n",
        "warehouse_eff = warehouse_summary.withColumn(\"shipping_to_revenue_ratio\",\n",
        "            round(col(\"total_shipping_cost\") / col(\"total_revenue\"), 3)) \\\n",
        "            .select(\"warehouse_id\", \"shipping_to_revenue_ratio\")\n",
        "\n",
        "warehouse_eff.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Delivery performance per warehouse: average delivery delay in days\n",
        "\n",
        "delivery_perf = logistics_fact.groupBy(\"warehouse_id\") \\\n",
        "    .agg(round(avg(\"delivery_time_delta_days\"), 2).alias(\"avg_delivery_time_delta\")) \\\n",
        "    .orderBy(\"warehouse_id\")\n",
        "\n",
        "delivery_perf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Product-level analysis of late deliveries:\n",
        "\n",
        "product_delay = logistics_fact_joined.groupBy(\"product_id\") \\\n",
        "    .agg(sum(\"is_late_delivery\").alias(\"late_deliveries\"),\n",
        "         countDistinct(\"order_id\").alias(\"total_orders\")) \\\n",
        "    .withColumn(\"late_rate\", round(col(\"late_deliveries\") / col(\"total_orders\") * 100, 2)) \\\n",
        "    .orderBy(desc(\"late_rate\"))\n",
        "\n",
        "product_delay.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "\n",
        "# Save Fact Table\n",
        "logistics_fact.write.mode(\"overwrite\").parquet(\"/staging_zone/staging_logistics_fact\")\n",
        "\n",
        "# Save Order Summary\n",
        "order_summary.write.mode(\"overwrite\").parquet(\"/staging_zone/order_summary\")\n",
        "\n",
        "# Save Warehouse Summary\n",
        "warehouse_summary.write.mode(\"overwrite\").parquet(\"/staging_zone/warehouse_summary\")\n",
        "\n",
        "# Save Product Delay\n",
        "product_delay.write.mode(\"overwrite\").parquet(\"/staging_zone/product_delay_summary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.pyspark\n",
        "# Register Temp Views for Spark SQL queries\n",
        "\n",
        "logistics_fact.createOrReplaceTempView(\"logistics_fact\")\n",
        "order_summary.createOrReplaceTempView(\"order_summary\")\n",
        "warehouse_summary.createOrReplaceTempView(\"warehouse_summary\")\n",
        "product_delay.createOrReplaceTempView(\"product_delay_summary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "-- Spark SQL Query: Calculate average delivery time delta (days) per warehouse\n",
        "\n",
        "SELECT warehouse_id,\n",
        "    ROUND(AVG(delivery_time_delta_days), 2) AS avg_delivery_time_delta\n",
        "FROM logistics_fact\n",
        "GROUP BY warehouse_id\n",
        "ORDER BY warehouse_id;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "\n",
        "-- Top warehouses with highest late delivery rate\n",
        "\n",
        "SELECT warehouse_id, late_delivery_rate\n",
        "FROM warehouse_summary\n",
        "ORDER BY late_delivery_rate DESC;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "\n",
        "-- Top 10 products causing delays\n",
        "\n",
        "SELECT product_id, late_rate, late_deliveries, total_orders\n",
        "FROM product_delay_summary\n",
        "ORDER BY late_rate DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "\n",
        "-- Revenue vs shipping cost ratio per warehouse\n",
        "\n",
        "SELECT warehouse_id,\n",
        "       total_revenue,\n",
        "       total_shipping_cost,\n",
        "       ROUND(total_shipping_cost/total_revenue,3) AS shipping_to_revenue_ratio\n",
        "FROM warehouse_summary\n",
        "ORDER BY shipping_to_revenue_ratio DESC;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark.sql\n",
        "\n",
        "-- Warehouse efficiency vs late deliveries\n",
        "\n",
        "SELECT \n",
        "    warehouse_id,\n",
        "    COUNT(order_id) AS total_orders,\n",
        "    SUM(is_late_delivery) AS late_deliveries,\n",
        "    ROUND(SUM(total_revenue_per_order), 2) AS total_revenue,\n",
        "    ROUND(SUM(shipping_cost), 2) AS total_shipping_cost,\n",
        "    ROUND(SUM(total_revenue_per_order)/SUM(shipping_cost), 2) AS revenue_to_shipping_ratio,\n",
        "    ROUND(SUM(is_late_delivery)/COUNT(order_id)*100, 2) AS late_delivery_rate_percentage\n",
        "FROM order_summary\n",
        "GROUP BY warehouse_id\n",
        "ORDER BY revenue_to_shipping_ratio DESC, late_delivery_rate_percentage ASC;"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Scala (Spark 2)",
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "Project"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
